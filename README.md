## Variation auto encoder using pytorch

In this project we will implement a variational auto encoder (VAE) using (de)convolutional neural networks ((d)CNN) using pytorch library, so to perform several image analysis tasks to the MNIST dataset of digits. We consider 50.000 training image dataset, 10.000 validation dataset and finally the last 10.000 image dataset for test purposes. The images have $28 \times 28$ dimensions and the pixel values are normalized in the range [0,1]. We transform the dataset into tensors and we pass them through a torch data-loader so to create data samples with batch-size of 256.

VAE consists of 2 parts: encoder and decoder. Both of these parts are neural networks with similar architectures. The encoder learns the distribution the parameters $\mu=\left(\mu_{1}, \ldots, \mu_{n}\right)^{T}=\mu(\mathbf{x})$ and $\sigma=\left(\sigma_{1}, \ldots, \sigma_{n}\right)^{T}=\sigma(\mathbf{x})$ in order to learn the probability distribution $q_{\phi}(Z \mid X)$ where $\mu(x)$ is the mean parameter and $\sigma(x)$ is the standard deviation of the given distribution, which can be expressed as $$q_{\phi}(Z \mid X) = \mathcal{N}\left(\mathbf{z} \mid \mu(X),\left(\sigma^{2}(X)\right)\right) = \prod_{i=1}^{D} \mathscr{N}\left(z_{i} \mid \mu_{i}, \sigma_{i}^{2}\right)$$.

The decoder is used to learn the parameters $\left(m_{1}, \ldots, m_{n}\right)^{T}=\mathbf{m}(\mathbf{z})$ and $\left(s_{1}, \ldots, s_{n}\right)^{T}= \mathbf{s}(\mathbf{z})$ of the distribution $p_{\theta}(X \mid Z)$ in order to learn the probability distribution $p_{\theta}(X \mid Z)$, which can be expressed as $$p_{\theta}(X \mid Z) = \mathcal{N}\left(\mathbf{x} \mid m(Z),\left(s(Z)^{2}\right)\right) = \prod_{i=1}^{D} \mathscr{N}\left(x_{i} \mid m_{i}, s_{i}^{2}\right)$$.

The loss function is expressed as an optimization (maximization) problem of the conditional log-likelihood, or the lower bound of it (ELBO) since it is very costly to compute the log-likelihood.

$$L \geq \mathbb{E}_{q(\mathbf{z} \mid \mathbf{x})} \left[ \log \frac{p(\mathbf{x} \mid \mathbf{z})}{q(\mathbf{z} \mid \mathbf{x})}\right]$$

It is proven that for maximizing the ELBO, meaning to be as close as possible to the true value, we need to minimize the KL-divergence since the difference of the $L - ELBO = K L(q(\mathbf{z} \mid \mathbf{x}) \| p(\mathbf{Z|x}))$. From that we use the equation bellow:

$$\log p(\mathbf{x}) = \mathbb{E}_{q(\mathbf{z} \mid \mathbf{x})}[\log p(\mathbf{x} \mid \mathbf{z})] + K L(q(\mathbf{Z} \mid \mathbf{x}) \| p(\mathbf{Z|x}))$$

Also, we are going to discuss and evaluate the variations of distributions that we used for our model. We tested Gaussian distribution, Beta distributions with [0, 1], Categorical distributions with discretised (binned) data,and Bernoulli distributions with re-interpreted data as probabilities for a given pixel to be black or white. In this specific task Bernoulli distribution (BD) is the most sufficient choice. Bernoulli distribution makes sense for black and white (i.e. binary) images. The Bernoulli distribution is binary, so it assumes that observations may only have two possible outcomes and this matches our dataset specification. Generalizing this statement we can see that when we use distributions with range [0, 1] we obtain better results because of the "nature" of our data. 

Finally, for the first part we will investigate the structure of the latent variables **Z**, and see how it captures structure that was implicitly present in the data. At first the latent space is two-dimensional, i.e. such that $\textbf{Z} = \mathbb{R}^2$. Then we evaluate the model in the first 1000 datapoints of our testing data, by using the encoder. We create a function that plots the $\mu(z_i)$ outputs of our encoder on a 2-dimensional plot, color-coded based on the labels of the digits $y_i$.